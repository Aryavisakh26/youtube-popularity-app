# -*- coding: utf-8 -*-
"""popularity predictor final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dsq3RfZhi8C_XTWVKcGieyn-9zKvwXRi
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv("/content/youtube_influencers_extended.csv")

df.head()

df.info()

df.describe()

df.columns

df.info()

# Check for duplicates
duplicates = df.duplicated().sum()
print(f"\nNumber of duplicate rows: {duplicates}")

# Remove duplicates
df = df.drop_duplicates()

#if duplicated rows present to drop them
df.drop_duplicates(inplace=True)

# check duplicate column
dup_cols=df.columns.duplicated()

df=df.loc[:,~dup_cols]

#Handling Missing Values
df.isna()

# Check for missing values
print("\nMissing values per column:")
print(df.isnull().sum())

sns.heatmap(df.isna())
plt.show()

# Drop columns that are not important or have too many missing values
# Custom_URL, Banner_Image, and Default_Language are optional ‚Äî removing them helps avoid noise
df = df.drop(columns=["Custom_URL", "Banner_Image", "Default_Language"], errors='ignore')

# 1Ô∏è‚É£ Fill categorical columns
df["Country"] = df["Country"].fillna("Unknown")
df["Profile_Country"] = df["Profile_Country"].fillna("Unknown")
df["Topic_Categories"] = df["Topic_Categories"].fillna("Miscellaneous")
df["Keywords"] = df["Keywords"].fillna("None")
df["Description"] = df["Description"].fillna("No Description")

# 2Ô∏è‚É£ Fill numeric columns
num_cols = df.select_dtypes(include=['int64', 'float64']).columns
df[num_cols] = df[num_cols].fillna(0)

# 3Ô∏è‚É£ Handle missing Popularity_Label (Target variable)
# Drop rows without target label, since we can‚Äôt train on them
df = df.dropna(subset=["Popularity_Label"])

df.isnull().sum()

print("Shape:", df.shape)
print("Columns:", df.columns.tolist())

# =====  FEATURE ENGINEERING =====
# Engagement ratio = Total_Views / Total_Videos
df["Engagement_Ratio"] = (df["Total_Views"] / (df["Total_Videos"] + 1)).replace([np.inf, -np.inf], 0)

# Subscriber to View ratio
df["Subscriber_to_View_Ratio"] = (df["Subscribers"] / (df["Total_Views"] + 1)).replace([np.inf, -np.inf], 0)

# Quality of engagement (Engagement per 1000 subscribers)
df["Engagement_Quality"] = (df["Comment_Count"] / (df["Subscribers"] + 1)) * 1000

# Comments per video
df["Comments_Per_Video"] = (df["Comment_Count"] / (df["Total_Videos"] + 1)).replace([np.inf, -np.inf], 0)

# Subscribers gained per year
df["Subscribers_Per_Year"] = (df["Subscribers"] / (df["Account_Age_Years"] + 0.1)).replace([np.inf, -np.inf], 0)

df.head()

# =====  EXPLORATORY DATA ANALYSIS (EDA) =====
plt.figure(figsize=(6,4))
sns.countplot(x="Popularity_Label", data=df)
plt.title("Popularity Level Distribution")
plt.show()

plt.figure(figsize=(10,6))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap="coolwarm")
plt.title("Feature Correlation Heatmap")
plt.show()

plt.figure(figsize=(6,4))
sns.scatterplot(x="Subscribers", y="Total_Views", hue="Popularity_Label", data=df)
plt.title("Subscribers vs Views")
plt.show()

# ===== ENCODE CATEGORICAL VARIABLES =====
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df["Country"] = le.fit_transform(df["Country"])
df["Keyword"] = le.fit_transform(df["Keyword"])
df["Topic_Categories"] = le.fit_transform(df["Topic_Categories"])
df["Popularity_Label"] = le.fit_transform(df["Popularity_Label"])  # Target variable

# =====  OUTLIER DETECTION & HANDLING =====

# Select numeric columns
numeric_cols = [
    "Subscribers", "Total_Views", "Total_Videos",
    "Comment_Count", "Account_Age_Years",
    "Comments_Per_Video", "Subscribers_Per_Year"
]

# Visual check using boxplots (optional)
for col in numeric_cols:
    plt.figure()
    plt.boxplot(df[col])
    plt.title(f"Boxplot of {col}")
    plt.show()

# Handle outliers using IQR (Interquartile Range)
for col in numeric_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Cap the outliers (winsorization)
    df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])
    df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])

print("‚úÖ Outliers capped using IQR method.")

df.shape

df.describe()

df.head()

features = [
    "Subscribers",
    "Total_Views",
    "Total_Videos",
    "Comment_Count",
    "Account_Age_Years",
    "Comments_Per_Video",
    "Subscribers_Per_Year",
]

X = df[features]
y = df["Popularity_Label"]

# ===== TRAIN-TEST SPLIT & SCALING =====
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("X_train shape:",X_train.shape)
print("X_test shape:",X_test.shape)
print("y_train shape:",y_train.shape)
print("y_test shape:",y_test.shape)

scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# =====  MODEL TRAINING =====
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.model_selection import cross_val_score

models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),
    "SVM": SVC(kernel='rbf', probability=True, random_state=42)
}

#Train and Evaluate Each Model
# --------------------------------------------
for name, model in models.items():
    print(f"\n================== {name} ==================")

    # Use scaled data for models that need it
    if name in ["Logistic Regression", "SVM"]:
        model.fit(X_train_scaled, y_train)
        y_train_pred = model.predict(X_train_scaled)
        y_test_pred = model.predict(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)

    # ---- Train Results ----
    train_acc = accuracy_score(y_train, y_train_pred)
    print("---- Train Results ----")
    print(f"Accuracy: {train_acc}")
    print(confusion_matrix(y_train, y_train_pred))

    # ---- Test Results ----
    test_acc = accuracy_score(y_test, y_test_pred)
    print("\n---- Test Results ----")
    print(f"Accuracy: {test_acc}")
    print(confusion_matrix(y_test, y_test_pred))
    print(classification_report(y_test, y_test_pred))

# Cross-validation for Best Model (Random Forest)
# --------------------------------------------
rf = RandomForestClassifier(random_state=42)
cv_scores = cross_val_score(rf, X_train, y_train, cv=5)
print(f"\nüîÅ Cross-Validation Accuracy (Random Forest): {cv_scores.mean():.4f}")

#Visualize Model Comparison
import matplotlib.pyplot as plt

model_names = ["LogReg", "DecisionTree", "RandomForest", "XGBoost", "SVM"]
train_acc = [0.970, 1.0, 1.0, 1.0, 0.977]
test_acc = [0.963, 0.978, 0.981, 0.984, 0.962]

plt.figure(figsize=(8,5))
plt.bar(model_names, test_acc, color='skyblue', label='Test Accuracy')
plt.bar(model_names, train_acc, color='orange', alpha=0.4, label='Train Accuracy')
plt.xlabel("Models")
plt.ylabel("Accuracy")
plt.title("Model Comparison - Train vs Test Accuracy")
plt.legend()
plt.show()







print("\nüîÅ Cross-Validation Accuracy (All Models):\n")
for name, model in models.items():
    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)
    print(f"{name}: {cv_scores.mean():.4f}")

from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Train the model
xgb_model = XGBClassifier(eval_metric='mlogloss', random_state=42)
xgb_model.fit(X_train_scaled, y_train)

# Predict on test set
y_pred = xgb_model.predict(X_test_scaled)

# Evaluate
print("===== XGBoost Classification Report =====")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

#  Hyperparameter Tuning (XGBoost)
from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier

# ‚úÖ Define the base model
xgb = XGBClassifier(eval_metric='mlogloss', random_state=42)

# ‚úÖ Define the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],     # Number of trees
    'max_depth': [3, 5, 7],              # Depth of each tree
    'learning_rate': [0.01, 0.1, 0.2],   # Step size shrinkage
    'subsample': [0.8, 1.0],             # Fraction of samples used per tree
    'colsample_bytree': [0.8, 1.0]       # Fraction of features used per tree
}

# ‚úÖ Grid Search
grid_search = GridSearchCV(
    estimator=xgb,
    param_grid=param_grid,
    scoring='accuracy',
    cv=3,
    verbose=2,
    n_jobs=-1
)

# ‚úÖ Fit on training data
grid_search.fit(X_train_scaled, y_train)

# ‚úÖ Print best parameters and best score
print("\nüîç Best Parameters from Grid Search:")
print(grid_search.best_params_)

print(f"\nüèÜ Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}")

# Train Final Tuned Model
best_params = grid_search.best_params_

best_xgb = XGBClassifier(
    **best_params,
    eval_metric='mlogloss',
    random_state=42
)

best_xgb.fit(X_train_scaled, y_train)
y_pred_best = best_xgb.predict(X_test_scaled)

# ‚úÖ Evaluate
from sklearn.metrics import accuracy_score, classification_report
acc = accuracy_score(y_test, y_pred_best)
print(f"\n‚úÖ Tuned XGBoost Model Accuracy: {acc:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred_best))

# ‚úÖ Confusion Matrix for Tuned Model
from sklearn.metrics import ConfusionMatrixDisplay
cm_final = confusion_matrix(y_test, y_pred_best)
disp = ConfusionMatrixDisplay(confusion_matrix=cm_final, display_labels=best_xgb.classes_)
disp.plot(cmap='Greens')
plt.title("Confusion Matrix - Tuned XGBoost Model")
plt.show()

#Feature Importance (for XGBoost)
import matplotlib.pyplot as plt
import seaborn as sns
from xgboost import plot_importance

# Plot XGBoost feature importance
plt.figure(figsize=(10, 6))
plot_importance(best_xgb, max_num_features=10, importance_type='gain')
plt.title("Top 10 Important Features (XGBoost)")
plt.show()

#Model Comparison Visualization
# Accuracy comparison dictionary (replace with your actual values)
model_scores = {
    "Logistic Regression": 0.9626,
    "Decision Tree": 0.9776,
    "Random Forest": 0.9808,
    "XGBoost": 0.9840,
    "SVM": 0.9615
}

plt.figure(figsize=(8, 5))
sns.barplot(x=list(model_scores.keys()), y=list(model_scores.values()), palette="viridis")
plt.title("Model Accuracy Comparison")
plt.ylabel("Accuracy")
plt.xticks(rotation=30)
plt.show()

#Visualize Confusion Matrix
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred_best)

plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix - Tuned XGBoost")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

import joblib

# Save model
joblib.dump(best_xgb, "best_xgb_model.pkl")
print("‚úÖ Model saved as best_xgb_model.pkl")

joblib.dump(scaler, "scaler.pkl")
print("‚úÖ Scaler saved as scaler.pkl")









